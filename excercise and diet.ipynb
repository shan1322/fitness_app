{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6278fa8e-c88a-4aca-8c62-2fd6c4023177",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import hamming_loss, accuracy_score, classification_report\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4259ba3-ee82-4745-aa68-d2b1e6dca409",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer, LabelEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def preprocess_for_model_with_drops(\n",
    "    file_path,\n",
    "    drop_diet_classes=None,\n",
    "    drop_exercise_classes=None,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    diet_model_path=\"diet_model.pkl\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Preprocess data for training and testing models with the ability to drop specific target classes.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the dataset.\n",
    "        drop_diet_classes (list): List of diet classes to drop.\n",
    "        drop_exercise_classes (list): List of exercise classes to drop.\n",
    "        test_size (float): Proportion of the dataset to include in the test split.\n",
    "        random_state (int): Random seed for reproducibility.\n",
    "        diet_model_path (str): Path to save the trained diet model.\n",
    "\n",
    "    Returns:\n",
    "        tuple: X_train, y_diet_train, X_test, y_diet_test, \n",
    "               diet_model_path, mlb_diet (MultiLabelBinarizer), scaler (StandardScaler), \n",
    "               mlb_exercises (if exercises exist).\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    df = pd.read_excel(file_path)\n",
    "\n",
    "    # Handle invalid numeric data\n",
    "    numeric_cols = [\"Age\", \"Height\", \"Weight\", \"BMI\"]\n",
    "    for col in numeric_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\").fillna(df[col].median())\n",
    "\n",
    "    # Process categorical data\n",
    "    categorical_cols = [\"Sex\", \"Level\", \"Fitness Goal\", \"Fitness Type\", \"Hypertension\", \"Diabetes\"]\n",
    "    label_encoders = {}\n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "\n",
    "    # Clean and split \"Diet\"\n",
    "    df[\"Diet\"] = (\n",
    "        df[\"Diet\"]\n",
    "        .str.replace(r\"[.,:(),;]\", \"\", regex=True)\n",
    "        .str.replace(r\"\\b(?:intake|and|or)\\b\", \"\", regex=True)\n",
    "        .str.strip()\n",
    "    )\n",
    "    df[\"Diet_labels\"] = df[\"Diet\"].str.split()\n",
    "\n",
    "    # Clean and split \"Exercises\" if present\n",
    "    mlb_exercises = None\n",
    "    if \"Exercises\" in df.columns:\n",
    "        df[\"Exercises\"] = (\n",
    "            df[\"Exercises\"]\n",
    "            .str.replace(r\"[.,:()]\", \"\", regex=True)\n",
    "            .str.replace(r\"\\b(?:and|or)\\b\", \"\", regex=True)\n",
    "            .str.strip()\n",
    "        )\n",
    "        df[\"Exercise_labels\"] = df[\"Exercises\"].str.split()\n",
    "\n",
    "        # Drop specific exercise classes if provided\n",
    "        if drop_exercise_classes:\n",
    "            df[\"Exercise_labels\"] = df[\"Exercise_labels\"].apply(\n",
    "                lambda x: [item for item in x if item not in drop_exercise_classes]\n",
    "            )\n",
    "\n",
    "        # Initialize MultiLabelBinarizer for exercises\n",
    "        mlb_exercises = MultiLabelBinarizer()\n",
    "        y_exercises = mlb_exercises.fit_transform(df[\"Exercise_labels\"])\n",
    "\n",
    "    # Handle \"chicken\" variations\n",
    "    df[\"Diet_labels\"] = df[\"Diet_labels\"].apply(\n",
    "        lambda x: [\"chicken\" if item.lower() in [\"chicken\", \"chickken\"] else item for item in x]\n",
    "    )\n",
    "\n",
    "    # Drop specific diet classes if provided\n",
    "    if drop_diet_classes:\n",
    "        df[\"Diet_labels\"] = df[\"Diet_labels\"].apply(\n",
    "            lambda x: [item for item in x if item not in drop_diet_classes]\n",
    "        )\n",
    "\n",
    "    # Initialize MultiLabelBinarizer for diet\n",
    "    mlb_diet = MultiLabelBinarizer()\n",
    "    y_diet = mlb_diet.fit_transform(df[\"Diet_labels\"])\n",
    "\n",
    "    # Combine numeric and categorical features\n",
    "    X_numeric = df[numeric_cols]\n",
    "    X_categorical = df[categorical_cols]\n",
    "\n",
    "    # Standardize numeric features\n",
    "    scaler = StandardScaler()\n",
    "    X_numeric_scaled = scaler.fit_transform(X_numeric)\n",
    "\n",
    "    # Combine scaled numeric and categorical features\n",
    "    X = np.hstack([X_numeric_scaled, X_categorical.values])\n",
    "\n",
    "    # Ensure `y_diet` matches the number of samples in `X`\n",
    "    assert len(X) == len(y_diet), \"X and y_diet must have the same number of samples.\"\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_diet_train, y_diet_test = train_test_split(\n",
    "        X, y_diet, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Include exercise binarizer and labels if present\n",
    "    if mlb_exercises is not None:\n",
    "        y_exercises_train, y_exercises_test = train_test_split(\n",
    "            y_exercises, test_size=test_size, random_state=random_state\n",
    "        )\n",
    "    return X_train,y_diet_train,X_test,y_diet_test,y_exercises_train,y_exercises_test,diet_model_path,mlb_diet,mlb_exercises,scaler,label_encoders\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "739c9181-92aa-42eb-9c99-fd59a3d5d38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "# Preprocess data\n",
    "file_path = \"data/excercise and diet/diet_gym.xlsx\"\n",
    "X_train,y_diet_train,X_test,y_diet_test,y_exercises_train,y_exercises_test,diet_model_path,mlb_diet,mlb_exercises,scaler,label_encoders = preprocess_for_model_with_drops(file_path,drop_diet_classes=[ \"law\",\n",
    "    \"coldpressed\",\n",
    "    \"juiceand\",\n",
    "    \"juicekale\",\n",
    "    \"protein\",\n",
    "    \"products\",\n",
    "    \"spandwich\",\n",
    "    \"standwish\",\"Intake\",\"Lettuce;\", \"Nuts;\", \"Onion;\", \"Seeds;\", \"Spandwich;\", \"Teff;\", \"chestnut;Protein\", \"legumes;\", \"Cattoge\", \"Icebetg\", \"Papper\", \"Standwish\", \"Spandwich;\", \"Law\", \"Diet\"\n",
    "]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbfa58b2-b963-4157-b001-df2bf0f6cca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "def save_preprocessor(scaler,label_dict,path,mlb_diet,mlb_excercises):\n",
    "    joblib.dump(scaler, os.path.join(path ,\"scaler.pkl\"))\n",
    "    joblib.dump(mlb_diet, os.path.join(path ,\"mlb_diet.pkl\"))\n",
    "    joblib.dump(mlb_excercises, os.path.join(path ,\"mlb_exercis.pkl\"))\n",
    "\n",
    "\n",
    "    new_dict={}\n",
    "    for name,model in label_dict.items():\n",
    "        file_name=name+\".pkl\"\n",
    "        joblib.dump(model,os.path.join(path,name+\".pkl\"))\n",
    "        new_dict[name]=file_name\n",
    "    with open(path+\"/model_paths.json\", \"w\") as f:\n",
    "        json.dump(new_dict, f)\n",
    "        \n",
    "    return _\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71cde845-d83e-420e-8429-c23d9523298b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_preprocessor(scaler,label_encoders,path=\"models/gym_diet/preprocessor\",mlb_diet=mlb_diet,mlb_excercises=mlb_exercises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bfe3a60d-dc20-4f5a-b4eb-94196cf9171b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape, output_shape):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(input_shape,)),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(output_shape, activation='sigmoid')  # Sigmoid for multi-label output\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "def train_and_save_model(X_train, y_train, X_test, y_test, output_path, mlb, scaler):\n",
    "    model = build_model(X_train.shape[1], y_train.shape[1])\n",
    "    model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "    print(\"Hamming Loss:\", hamming_loss(y_test, y_pred))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred, target_names=mlb.classes_))\n",
    "\n",
    "    # Save the model and preprocessing objects\n",
    "    model.save(output_path)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2bc7c5fc-e4eb-442c-97a1-cfa248efbbdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m292/292\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 6.7960e-04 - loss: 0.4402 - val_accuracy: 0.0000e+00 - val_loss: 0.1417\n",
      "Epoch 2/20\n",
      "\u001b[1m292/292\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.0141 - loss: 0.1507 - val_accuracy: 0.0000e+00 - val_loss: 0.0983\n",
      "Epoch 3/20\n",
      "\u001b[1m292/292\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.0112 - loss: 0.1150 - val_accuracy: 0.0000e+00 - val_loss: 0.0844\n",
      "Epoch 4/20\n",
      "\u001b[1m292/292\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.0057 - loss: 0.0964 - val_accuracy: 0.0000e+00 - val_loss: 0.0783\n",
      "Epoch 5/20\n",
      "\u001b[1m292/292\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.0033 - loss: 0.0875 - val_accuracy: 0.0000e+00 - val_loss: 0.0764\n",
      "Epoch 6/20\n",
      "\u001b[1m292/292\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.0028 - loss: 0.0828 - val_accuracy: 0.0000e+00 - val_loss: 0.0740\n",
      "Epoch 7/20\n",
      "\u001b[1m292/292\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.0014 - loss: 0.0779 - val_accuracy: 0.0000e+00 - val_loss: 0.0711\n",
      "Epoch 8/20\n",
      "\u001b[1m292/292\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.0015 - loss: 0.0773 - val_accuracy: 0.0000e+00 - val_loss: 0.0705\n",
      "Epoch 9/20\n",
      "\u001b[1m292/292\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.0016 - loss: 0.0782 - val_accuracy: 0.0000e+00 - val_loss: 0.0700\n",
      "Epoch 10/20\n",
      "\u001b[1m292/292\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.0010 - loss: 0.0723 - val_accuracy: 0.0000e+00 - val_loss: 0.0686\n",
      "Epoch 11/20\n",
      "\u001b[1m292/292\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 8.3413e-04 - loss: 0.0742 - val_accuracy: 0.0000e+00 - val_loss: 0.0665\n",
      "Epoch 12/20\n",
      "\u001b[1m292/292\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 8.6973e-04 - loss: 0.0716 - val_accuracy: 0.0000e+00 - val_loss: 0.0657\n",
      "Epoch 13/20\n",
      "\u001b[1m292/292\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.0013 - loss: 0.0712 - val_accuracy: 0.0000e+00 - val_loss: 0.0649\n",
      "Epoch 14/20\n",
      "\u001b[1m292/292\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 5.0260e-04 - loss: 0.0666 - val_accuracy: 0.0000e+00 - val_loss: 0.0640\n",
      "Epoch 15/20\n",
      "\u001b[1m292/292\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.0011 - loss: 0.0665 - val_accuracy: 0.0000e+00 - val_loss: 0.0639\n",
      "Epoch 16/20\n",
      "\u001b[1m292/292\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 5.3405e-04 - loss: 0.0676 - val_accuracy: 0.0000e+00 - val_loss: 0.0630\n",
      "Epoch 17/20\n",
      "\u001b[1m292/292\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 2.7725e-04 - loss: 0.0637 - val_accuracy: 0.0000e+00 - val_loss: 0.0617\n",
      "Epoch 18/20\n",
      "\u001b[1m292/292\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 6.6787e-04 - loss: 0.0665 - val_accuracy: 0.0000e+00 - val_loss: 0.0625\n",
      "Epoch 19/20\n",
      "\u001b[1m292/292\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.0016 - loss: 0.0662 - val_accuracy: 0.0000e+00 - val_loss: 0.0614\n",
      "Epoch 20/20\n",
      "\u001b[1m292/292\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.0020 - loss: 0.0685 - val_accuracy: 0.0000e+00 - val_loss: 0.0609\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "Hamming Loss: 0.024022467777796354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "           Aloe       0.93      0.93      0.93       450\n",
      "          Apple       0.97      0.97      0.97      2059\n",
      "           Baru       0.98      0.98      0.98      2345\n",
      "          Beech       0.96      0.97      0.97      1895\n",
      "       Beetroot       0.95      0.95      0.95      1354\n",
      "       Broccoli       0.93      0.93      0.93       450\n",
      "         Capers       0.90      0.92      0.91       541\n",
      "        Carrots       0.92      0.92      0.92       631\n",
      "         Cheese       0.96      0.97      0.97      2018\n",
      "   Cold-pressed       0.93      0.93      0.93       450\n",
      "          Fruit       0.92      0.92      0.92       631\n",
      "         Garlic       0.97      0.97      0.97      2059\n",
      "          Green       0.94      0.96      0.95      1599\n",
      "           Hemp       0.95      0.95      0.95      1193\n",
      "        Iceberg       0.90      0.92      0.91       541\n",
      "          Juice       0.99      1.00      1.00      2903\n",
      "        Lettuce       0.97      0.98      0.97      2199\n",
      "          Mango       0.96      0.95      0.95      1354\n",
      "           Milk       0.93      0.93      0.93       450\n",
      "          Mixed       0.91      0.94      0.92       754\n",
      "       Mushroom       0.95      0.94      0.94      1027\n",
      "           Nuts       0.98      0.98      0.98      2345\n",
      "          Onion       0.93      0.93      0.93       450\n",
      "         Potato       0.90      0.87      0.89       181\n",
      "        Protein       0.98      0.98      0.98      2363\n",
      "            Red       0.90      0.87      0.89       181\n",
      "           Roma       0.90      0.92      0.91       541\n",
      "       Sandwich       0.90      0.92      0.91       359\n",
      "          Seeds       0.95      0.96      0.96      1734\n",
      "           Skim       0.93      0.93      0.93       450\n",
      "      Spandwich       0.95      0.94      0.94      1027\n",
      "        Spinach       0.93      0.93      0.93       450\n",
      "         Squash       0.90      0.92      0.91       541\n",
      "          Sweet       0.90      0.87      0.89       181\n",
      "           Teff       0.90      0.92      0.91       541\n",
      "       Tomatoes       0.91      0.93      0.92       705\n",
      "     Vegetables       0.99      1.00      1.00      2903\n",
      "     Watermelon       0.93      0.93      0.93       450\n",
      "          apple       0.91      0.91      0.91       394\n",
      "       beetroot       0.91      0.93      0.92       705\n",
      "           bell       0.92      0.92      0.92       377\n",
      "          black       0.93      0.84      0.89       161\n",
      "       broccoli       0.93      0.90      0.91       164\n",
      "         butter       0.90      0.92      0.91       359\n",
      "         carrot       0.90      0.87      0.89       181\n",
      "        carrots       0.92      0.92      0.92       377\n",
      "         celery       0.92      0.92      0.92       213\n",
      "         cheese       0.93      0.93      0.93       450\n",
      "         cherry       0.92      0.92      0.92       213\n",
      "chestnutProtein       0.93      0.86      0.89       161\n",
      "        chicken       0.92      0.92      0.92       213\n",
      "       cucumber       0.92      0.92      0.92       213\n",
      "      cucumbers       0.92      0.92      0.92       213\n",
      "          dairy       0.91      0.89      0.90       345\n",
      "           eggs       0.90      0.87      0.89       181\n",
      "            fat       0.93      0.93      0.93       450\n",
      "           fish       0.92      0.92      0.92       558\n",
      "          green       0.92      0.90      0.91       327\n",
      "         greens       0.92      0.92      0.92       377\n",
      "          jelly       0.90      0.92      0.91       359\n",
      "          juice       0.99      0.99      0.99      2690\n",
      "          leafy       0.93      0.90      0.91       164\n",
      "        legumes       0.92      0.92      0.92       558\n",
      "        low-fat       0.93      0.90      0.91       164\n",
      "          mango       0.93      0.94      0.93       886\n",
      "          meats       0.90      0.87      0.89       181\n",
      "       mushroon       0.92      0.90      0.91       327\n",
      "           nuts       0.90      0.87      0.89       181\n",
      "         papper       0.93      0.86      0.89       161\n",
      "  papperProtein       0.90      0.91      0.90       166\n",
      "         peanut       0.90      0.92      0.91       359\n",
      "        peppers       0.92      0.92      0.92       377\n",
      " peppersProtein       0.92      0.92      0.92       213\n",
      "        poultry       0.91      0.89      0.90       345\n",
      "       sandwich       0.90      0.92      0.91       359\n",
      "        spinach       0.92      0.92      0.92       213\n",
      "           tofu       0.92      0.92      0.92       377\n",
      "       tomatoes       0.92      0.92      0.92       213\n",
      "           vera       0.93      0.93      0.93       450\n",
      "         walnut       0.93      0.84      0.88       161\n",
      "          water       0.93      0.86      0.89       161\n",
      "     watermelon       0.90      0.87      0.89       181\n",
      "\n",
      "      micro avg       0.95      0.95      0.95     58183\n",
      "      macro avg       0.93      0.92      0.92     58183\n",
      "   weighted avg       0.95      0.95      0.95     58183\n",
      "    samples avg       0.95      0.95      0.95     58183\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # Train diet model\n",
    "diet_model_path = \"models/gym_diet/neural_nets/diet_model.h5\"\n",
    "diet_model = train_and_save_model(X_train, y_diet_train, X_test, y_diet_test, diet_model_path, mlb_diet, scaler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40eef4d2-19a5-4528-b53d-a95868c732d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m292/292\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.1048 - loss: 0.3986 - val_accuracy: 0.0522 - val_loss: 0.0333\n",
      "Epoch 2/20\n",
      "\u001b[1m292/292\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1147 - loss: 0.0348 - val_accuracy: 0.0171 - val_loss: 0.0152\n",
      "Epoch 3/20\n",
      "\u001b[1m292/292\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1550 - loss: 0.0145 - val_accuracy: 0.0069 - val_loss: 0.0132\n",
      "Epoch 4/20\n",
      "\u001b[1m292/292\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1264 - loss: 0.0117 - val_accuracy: 0.0694 - val_loss: 0.0115\n",
      "Epoch 5/20\n",
      "\u001b[1m292/292\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1474 - loss: 0.0099 - val_accuracy: 0.1096 - val_loss: 0.0113\n",
      "Epoch 6/20\n",
      "\u001b[1m292/292\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1382 - loss: 0.0097 - val_accuracy: 0.0994 - val_loss: 0.0126\n",
      "Epoch 7/20\n",
      "\u001b[1m292/292\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1578 - loss: 0.0087 - val_accuracy: 0.0895 - val_loss: 0.0124\n",
      "Epoch 8/20\n",
      "\u001b[1m292/292\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1454 - loss: 0.0067 - val_accuracy: 0.0685 - val_loss: 0.0117\n",
      "Epoch 9/20\n",
      "\u001b[1m292/292\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1511 - loss: 0.0063 - val_accuracy: 0.0570 - val_loss: 0.0111\n",
      "Epoch 10/20\n",
      "\u001b[1m292/292\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1335 - loss: 0.0068 - val_accuracy: 0.0839 - val_loss: 0.0118\n",
      "Epoch 11/20\n",
      "\u001b[1m292/292\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1478 - loss: 0.0077 - val_accuracy: 0.0874 - val_loss: 0.0120\n",
      "Epoch 12/20\n",
      "\u001b[1m292/292\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1554 - loss: 0.0065 - val_accuracy: 0.0274 - val_loss: 0.0112\n",
      "Epoch 13/20\n",
      "\u001b[1m292/292\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1382 - loss: 0.0092 - val_accuracy: 0.0912 - val_loss: 0.0106\n",
      "Epoch 14/20\n",
      "\u001b[1m292/292\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1554 - loss: 0.0074 - val_accuracy: 0.1062 - val_loss: 0.0113\n",
      "Epoch 15/20\n",
      "\u001b[1m292/292\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1535 - loss: 0.0074 - val_accuracy: 0.1054 - val_loss: 0.0131\n",
      "Epoch 16/20\n",
      "\u001b[1m292/292\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1519 - loss: 0.0083 - val_accuracy: 0.1045 - val_loss: 0.0116\n",
      "Epoch 17/20\n",
      "\u001b[1m292/292\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1565 - loss: 0.0081 - val_accuracy: 0.1199 - val_loss: 0.0109\n",
      "Epoch 18/20\n",
      "\u001b[1m292/292\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1517 - loss: 0.0088 - val_accuracy: 0.1028 - val_loss: 0.0108\n",
      "Epoch 19/20\n",
      "\u001b[1m292/292\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1574 - loss: 0.0050 - val_accuracy: 0.0047 - val_loss: 0.0121\n",
      "Epoch 20/20\n",
      "\u001b[1m292/292\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1365 - loss: 0.0060 - val_accuracy: 0.1126 - val_loss: 0.0105\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming Loss: 0.0024845784784098698\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       Brisk       0.99      1.00      0.99       774\n",
      "      Squats       1.00      1.00      1.00      1441\n",
      "    Swimming       1.00      0.99      1.00       370\n",
      "     Walking       1.00      0.99      1.00       370\n",
      "        Yoga       1.00      0.99      1.00       370\n",
      "       bench       1.00      1.00      1.00      1441\n",
      "       brisk       1.00      1.00      1.00       333\n",
      "     cycling       0.99      1.00      1.00      1107\n",
      "     dancing       0.99      1.00      1.00      1107\n",
      "   deadlifts       1.00      1.00      1.00      1441\n",
      "    overhead       1.00      1.00      1.00      1441\n",
      "     presses       1.00      1.00      1.00      1441\n",
      "     running       0.99      1.00      0.99       774\n",
      "    swimming       0.99      1.00      1.00      1107\n",
      "     walking       0.99      1.00      1.00      1107\n",
      "        yoga       1.00      1.00      1.00       725\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     15349\n",
      "   macro avg       1.00      1.00      1.00     15349\n",
      "weighted avg       1.00      1.00      1.00     15349\n",
      " samples avg       1.00      1.00      1.00     15349\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # Train exercise model\n",
    "exercise_model_path = \"models/gym_diet/neural_nets/exercise_model.h5\"\n",
    "exercise_model = train_and_save_model(X_train, y_exercises_train, X_test, y_exercises_test, exercise_model_path, mlb_exercises, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f265d15f-8cea-4112-93b8-c41a708ebee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_preprocessors(path,model_name):\n",
    "    preprocessor_path=path+\"/preprocessor/\"\n",
    "    model_path=path+\"/neural_nets/{}_model.h5\".format(model_name)\n",
    "    model = load_model(model_path)\n",
    "    scaler=joblib.load(preprocessor_path+\"scaler.pkl\")\n",
    "    with open(preprocessor_path+\"model_paths.json\", \"r\") as file:\n",
    "        label_dict = json.load(file) \n",
    "    for name,file_name in label_dict.items():\n",
    "        label_dict[name]=joblib.load(preprocessor_path+file_name)\n",
    "    mlb=joblib.load(preprocessor_path+\"mlb_\"+model_name+\".pkl\")\n",
    "\n",
    "    #mlb = joblib.load(model_path + \"_mlb.pkl\")\n",
    "    #scaler = joblib.load(model_path + \"_scaler.pkl\")\n",
    "    return model, mlb, scaler,label_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_top_recommendations(predictions, mlb, top_n=3):\n",
    "    recommended_items = []\n",
    "    for pred in predictions:\n",
    "        top_indices = np.argsort(pred)[-top_n:][::-1]  # Get indices of top N values\n",
    "        top_items = [mlb.classes_[i] for i in top_indices if pred[i] > 0.5]  # Include only if above threshold\n",
    "        recommended_items.append(top_items)\n",
    "    return recommended_items\n",
    "def predict_with_model(model, scaler, mlb, input_data,label_encoder, top_n=3):\n",
    "    scaler_data=np.asarray([input_data[0][:4]])\n",
    "    input_data_scaled = scaler.transform(scaler_data)\n",
    "    label_data=input_data[0][4:]\n",
    "    label_data_encoded=[]\n",
    "    count=0\n",
    "    for key,value in label_encoder.items():\n",
    "        data_point=np.asarray([label_data[count]])\n",
    "        label_data_encoded.append(value.transform(data_point))\n",
    "        count=count+1\n",
    "    label_data_encoded=[i[0] for i in label_data_encoded]\n",
    "    ids=list(input_data_scaled[0])\n",
    "    ids.extend(label_data_encoded)\n",
    "    final_data=np.asarray(ids)\n",
    "    final_data=final_data.reshape(1,10)\n",
    "\n",
    "    predictions = model.predict(final_data)\n",
    "    return get_top_recommendations(predictions, mlb, top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9debe45-3321-4162-8ede-4ce279c48015",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "D:\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diet recommendations for sample inputs: [['Juice', 'Vegetables', 'Protein']]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "Exercise recommendations for sample inputs: [['presses', 'overhead', 'deadlifts']]\n"
     ]
    }
   ],
   "source": [
    " # Load and predict diet recommendations\n",
    "path=\"models/gym_diet\"\n",
    "model_name=\"diet\"\n",
    "model, mlb, scaler,label_dict=load_model_and_preprocessors(path,model_name)\n",
    "sample_input = np.asarray([[18,1.68,47.5,16.83,\"Male\",\"Underweight\",\"Weight Gain\",\"Muscular Fitness\",\"No\",\"No\"]])\n",
    "diet_recommendations = predict_with_model(model, scaler, mlb, sample_input, top_n=3,label_encoder=label_dict)\n",
    "print(\"Diet recommendations for sample inputs:\", diet_recommendations)\n",
    "\n",
    "model_name=\"exercise\"\n",
    "model, mlb, scaler,label_dict=load_model_and_preprocessors(path,model_name)\n",
    "sample_input = np.asarray([[18,1.68,47.5,16.83,\"Male\",\"Underweight\",\"Weight Gain\",\"Muscular Fitness\",\"No\",\"No\"]])\n",
    "exer_recommendations = predict_with_model(model, scaler, mlb, sample_input, top_n=3,label_encoder=label_dict)\n",
    "print(\"Exercise recommendations for sample inputs:\", exer_recommendations)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955170cf-5c7e-4d55-b36b-e47185a311eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
